import time
from copy import deepcopy
from dataclasses import dataclass
from .settings import Settings
from langchain_together import ChatTogether
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import AIMessage


@dataclass
class TokenUsage:
    """Tracks token usage statistics for LLM interactions."""

    inputs: int
    """The number of input tokens used in the LLM interaction."""

    outputs: int
    """The number of output tokens generated by the LLM."""

    @classmethod
    def from_metadata(cls, metadata: dict):
        """Creates a TokenUsage instance from LLM metadata.

        Args:
            metadata (dict): Dictionary containing token usage information with
                keys 'input_tokens' and 'output_tokens'.

        Returns:
            TokenUsage: A new TokenUsage instance populated from the metadata.
        """
        return TokenUsage(inputs=metadata['input_tokens'], outputs=metadata['output_tokens'])


@dataclass
class LLMResult:
    """Stores the results of an LLM interaction including content and performance metrics."""

    content: str
    """The textual output generated by the LLM."""

    duration: float
    """The time taken for the interaction in seconds."""

    token_usage: TokenUsage
    """Object containing token usage statistics."""

    @classmethod
    def from_message(cls, message: AIMessage, duration: float):
        """Creates an LLMResult from an AIMessage and duration.

        Args:
            message (AIMessage): The message object returned by the LLM.
            duration (float): The time taken for the interaction in seconds.

        Returns:
            LLMResult: A new LLMResult instance populated from the message and duration.
        """
        return LLMResult(content=message.content, duration=duration, token_usage=TokenUsage.from_metadata(message.usage_metadata))


class LLMClient:
    """Client for interacting with multiple LLM models with prompt management."""

    def __init__(self, from_settings: Settings | None = None):
        """Initializes the LLMClient with models from settings.

        Args:
            from_settings (Settings | None): Optional custom settings to use instead of default.
                If None, uses the default settings.
        """
        used_config = deepcopy(
            from_settings.config) if from_settings else Settings()
        self.__models = [
            ChatTogether(
                model=model.id,
                temperature=0,
                max_tokens=None,
                together_api_key=used_config.api_key
            )
            for model in used_config.llm_models
        ]
        self.names = [model.name for model in used_config.llm_models]
        """Names of the available LLM models."""

        self.n_models = len(self.__models)
        """Number of available LLM models."""

        self.__prompt_key = None

    def inject_prompt(self, key: str, prompt: ChatPromptTemplate):
        """Injects a prompt template to be used with the LLM models.

        Args:
            key (str): Unique identifier for the prompt to avoid unnecessary re-initialization.
            prompt (ChatPromptTemplate): The prompt template to use with the LLM models.

        Note:
            If the same key is provided multiple times, the prompt will only be set once.
        """
        if self.__prompt_key == key:
            return
        self.__prompt_key = key
        self.__chains = [
            prompt | model
            for model in self.__models
        ]

    def run(self, values: dict, which_model: list[int] = []):
        """Executes the LLM interaction with the provided input values.

        Args:
            values (dict): Input values to be used with the injected prompt template.
            which_model (list[int]): Optional list of model indices to use. If empty,
                all available models will be used.

        Returns:
            dict[int, LLMResult]: Dictionary mapping model indices to their results.

        Raises:
            ValueError: If any model index in which_model is out of bounds.
            NotImplementedError: If no prompt has been injected before running.
        """
        n = len(which_model)
        used_model_idxs = which_model[:] if n > 0 else list(
            range(self.n_models))
        for i in used_model_idxs:
            if i < 0 or i >= self.n_models:
                raise ValueError(
                    f"Invalid model index: {i}. Must be between 0 and {len(self.n_models) - 1}.")
        if not hasattr(self, '_LLMClient__chains'):
            raise NotImplementedError(
                'There is no prompt injected yet. Please inject the prompt first')
        results: dict[int, LLMResult] = {}
        for i in used_model_idxs:
            time.sleep(1)
            start_time = time.time()
            result = self.__chains[i].invoke(values)
            end_time = time.time()
            duration = end_time - start_time
            results[i] = LLMResult.from_message(result, duration)
        return results
